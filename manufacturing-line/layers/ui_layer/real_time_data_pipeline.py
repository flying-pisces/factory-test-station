"""
Real-Time Data Pipeline - Week 13: UI & Visualization Layer

High-performance real-time data pipeline for delivering manufacturing data
to user interfaces with low latency and high reliability.
"""

import asyncio
import json
import logging
import time
import websocket
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable, Union
import threading
import queue
import random
import uuid


class DataPipelineStatus:
    """Data pipeline status constants."""
    INITIALIZING = "initializing"
    ACTIVE = "active"
    PAUSED = "paused"
    ERROR = "error"
    SHUTDOWN = "shutdown"


class RealTimeDataPipeline:
    """
    High-performance real-time data pipeline for manufacturing UI.
    
    Provides low-latency data streaming from AI engines and manufacturing
    systems to user interfaces with WebSocket connections, data buffering,
    and automatic failover capabilities.
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize Real-Time Data Pipeline.
        
        Args:
            config: Configuration dictionary for data pipeline settings
        """
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        
        # Performance targets
        self.data_latency_target_ms = 50      # Data delivery under 50ms
        self.websocket_response_target_ms = 25  # WebSocket response under 25ms
        self.buffer_processing_target_ms = 10   # Buffer processing under 10ms
        
        # Pipeline configuration
        self.max_connections = self.config.get('max_connections', 1000)
        self.buffer_size = self.config.get('buffer_size', 10000)
        self.batch_size = self.config.get('batch_size', 100)
        self.update_frequency_hz = self.config.get('update_frequency_hz', 20)
        
        # Connection management
        self.websocket_connections = {}
        self.connection_subscriptions = {}
        self.active_sessions = {}
        
        # Data management
        self.data_sources = {}
        self.data_buffers = {}
        self.data_processors = {}
        
        # Performance metrics
        self.pipeline_metrics = {
            'messages_processed': 0,
            'bytes_transferred': 0,
            'active_connections': 0,
            'avg_latency_ms': 0.0,
            'throughput_msgs_per_sec': 0.0,
            'error_count': 0,
            'uptime_seconds': 0
        }
        
        # Thread management
        self.lock = threading.Lock()
        self.executor = ThreadPoolExecutor(max_workers=8, thread_name_prefix="pipeline")
        self.processing_queues = {}
        self.worker_threads = {}
        
        # Pipeline status
        self.status = DataPipelineStatus.INITIALIZING
        self.start_time = datetime.now()
        
        # Initialize pipeline components
        self._initialize_data_pipeline()
        
        self.logger.info("RealTimeDataPipeline initialized successfully")
    
    def _initialize_data_pipeline(self):
        """Initialize data pipeline components."""
        try:
            # Initialize data source connections
            self.data_sources = {
                'ai_layer': {
                    'connection_type': 'internal',
                    'update_frequency': 10,  # 10 Hz
                    'data_types': ['predictions', 'alerts', 'insights', 'metrics'],
                    'buffer_size': 1000,
                    'status': 'active'
                },
                'production_system': {
                    'connection_type': 'internal',
                    'update_frequency': 20,  # 20 Hz
                    'data_types': ['throughput', 'quality', 'status', 'alarms'],
                    'buffer_size': 5000,
                    'status': 'active'
                },
                'equipment_monitoring': {
                    'connection_type': 'internal', 
                    'update_frequency': 50,  # 50 Hz
                    'data_types': ['sensor_data', 'health_status', 'diagnostics'],
                    'buffer_size': 10000,
                    'status': 'active'
                },
                'quality_systems': {
                    'connection_type': 'internal',
                    'update_frequency': 5,   # 5 Hz
                    'data_types': ['test_results', 'defects', 'compliance'],
                    'buffer_size': 1000,
                    'status': 'active'
                }
            }
            
            # Initialize data buffers for each source
            for source_name, source_config in self.data_sources.items():
                buffer_size = source_config['buffer_size']
                self.data_buffers[source_name] = queue.Queue(maxsize=buffer_size)
                self.processing_queues[source_name] = queue.Queue(maxsize=buffer_size * 2)
            
            # Initialize data processors
            self.data_processors = {
                'ai_predictions': self._process_ai_predictions,
                'production_metrics': self._process_production_metrics,
                'equipment_status': self._process_equipment_status,
                'quality_data': self._process_quality_data,
                'alerts': self._process_alerts
            }
            
            # Start data processing workers
            self._start_processing_workers()
            
            # Set pipeline status
            self.status = DataPipelineStatus.ACTIVE
            
            self.logger.info("Data pipeline components initialized")
            
        except Exception as e:
            self.status = DataPipelineStatus.ERROR
            self.logger.error(f"Failed to initialize data pipeline: {e}")
            raise
    
    def _start_processing_workers(self):
        """Start background worker threads for data processing."""
        for source_name in self.data_sources:
            worker_thread = threading.Thread(
                target=self._data_processing_worker,
                args=(source_name,),
                daemon=True,
                name=f"pipeline_worker_{source_name}"
            )
            worker_thread.start()
            self.worker_threads[source_name] = worker_thread
    
    def _data_processing_worker(self, source_name: str):
        """Background worker for processing data from specific source."""
        processing_queue = self.processing_queues[source_name]
        
        while self.status in [DataPipelineStatus.ACTIVE, DataPipelineStatus.INITIALIZING]:
            try:
                # Get data from processing queue with timeout
                try:
                    data_item = processing_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                # Process data item
                processed_data = self._process_data_item(source_name, data_item)
                
                # Distribute to subscribers
                if processed_data:
                    asyncio.run_coroutine_threadsafe(
                        self._distribute_data(source_name, processed_data),
                        asyncio.new_event_loop()
                    )
                
                # Update metrics
                with self.lock:
                    self.pipeline_metrics['messages_processed'] += 1
                
            except Exception as e:
                with self.lock:
                    self.pipeline_metrics['error_count'] += 1
                self.logger.error(f"Data processing error in {source_name}: {e}")
    
    async def register_websocket_connection(self, connection_id: str, 
                                          websocket_connection: Any,
                                          subscription_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Register new WebSocket connection for real-time data streaming.
        
        Args:
            connection_id: Unique connection identifier
            websocket_connection: WebSocket connection object
            subscription_config: Data subscription configuration
        
        Returns:
            Registration result with connection details
        """
        start_time = time.time()
        
        try:
            # Validate subscription configuration
            if not self._validate_subscription_config(subscription_config):
                raise ValueError("Invalid subscription configuration")
            
            # Register connection
            connection_info = {
                'connection_id': connection_id,
                'websocket': websocket_connection,
                'subscriptions': subscription_config.get('data_sources', []),
                'filters': subscription_config.get('filters', {}),
                'update_frequency': subscription_config.get('update_frequency', 20),
                'compression': subscription_config.get('compression', False),
                'registered_at': datetime.now().isoformat(),
                'last_activity': datetime.now().isoformat(),
                'status': 'active',
                'messages_sent': 0,
                'bytes_sent': 0
            }
            
            self.websocket_connections[connection_id] = connection_info
            
            # Set up subscriptions
            for data_source in connection_info['subscriptions']:
                if data_source not in self.connection_subscriptions:
                    self.connection_subscriptions[data_source] = []
                self.connection_subscriptions[data_source].append(connection_id)
            
            registration_time = (time.time() - start_time) * 1000
            
            # Update metrics
            with self.lock:
                self.pipeline_metrics['active_connections'] += 1
            
            # Performance validation
            if registration_time > self.websocket_response_target_ms:
                self.logger.warning(f"WebSocket registration exceeded target: {registration_time:.1f}ms")
            
            self.logger.info(f"Registered WebSocket connection: {connection_id} in {registration_time:.1f}ms")
            
            return {
                'success': True,
                'connection_id': connection_id,
                'registration_time_ms': registration_time,
                'subscriptions': connection_info['subscriptions']
            }
            
        except Exception as e:
            registration_time = (time.time() - start_time) * 1000
            with self.lock:
                self.pipeline_metrics['error_count'] += 1
            self.logger.error(f"Failed to register WebSocket connection: {e}")
            raise
    
    async def push_data_to_pipeline(self, source_name: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Push new data into the pipeline for processing and distribution.
        
        Args:
            source_name: Data source identifier
            data: Data payload to process and distribute
        
        Returns:
            Processing result with performance metrics
        """
        start_time = time.time()
        
        try:
            if source_name not in self.data_sources:
                raise ValueError(f"Unknown data source: {source_name}")
            
            if self.status != DataPipelineStatus.ACTIVE:
                raise RuntimeError(f"Pipeline not active: {self.status}")
            
            # Add metadata to data
            enriched_data = {
                'source': source_name,
                'timestamp': datetime.now().isoformat(),
                'data_id': str(uuid.uuid4()),
                'payload': data
            }
            
            # Add to processing queue
            processing_queue = self.processing_queues[source_name]
            
            try:
                processing_queue.put_nowait(enriched_data)
            except queue.Full:
                # Remove oldest item and add new one
                try:
                    processing_queue.get_nowait()
                    processing_queue.put_nowait(enriched_data)
                    self.logger.warning(f"Processing queue full for {source_name}, dropped oldest item")
                except queue.Empty:
                    pass
            
            processing_time = (time.time() - start_time) * 1000
            
            # Update metrics
            with self.lock:
                self.pipeline_metrics['bytes_transferred'] += len(str(data))
            
            # Performance validation
            if processing_time > self.buffer_processing_target_ms:
                self.logger.warning(f"Data buffering exceeded target: {processing_time:.1f}ms")
            
            return {
                'success': True,
                'source': source_name,
                'data_id': enriched_data['data_id'],
                'processing_time_ms': processing_time,
                'queue_size': processing_queue.qsize()
            }
            
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            with self.lock:
                self.pipeline_metrics['error_count'] += 1
            self.logger.error(f"Failed to push data to pipeline: {e}")
            raise
    
    def _process_data_item(self, source_name: str, data_item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Process individual data item."""
        try:
            payload = data_item['payload']
            
            # Apply source-specific processing
            if source_name == 'ai_layer':
                return self._process_ai_predictions(payload)
            elif source_name == 'production_system':
                return self._process_production_metrics(payload)
            elif source_name == 'equipment_monitoring':
                return self._process_equipment_status(payload)
            elif source_name == 'quality_systems':
                return self._process_quality_data(payload)
            else:
                return self._process_generic_data(payload)
                
        except Exception as e:
            self.logger.error(f"Data processing error: {e}")
            return None
    
    def _process_ai_predictions(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process AI prediction data."""
        return {
            'type': 'ai_prediction',
            'data': data,
            'timestamp': datetime.now().isoformat(),
            'priority': self._determine_priority(data, 'ai'),
            'requires_action': data.get('confidence', 0) > 0.8
        }
    
    def _process_production_metrics(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process production metrics data."""
        return {
            'type': 'production_metric',
            'data': data,
            'timestamp': datetime.now().isoformat(),
            'trend': self._calculate_trend(data),
            'alert_level': self._determine_alert_level(data)
        }
    
    def _process_equipment_status(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process equipment status data."""
        return {
            'type': 'equipment_status',
            'data': data,
            'timestamp': datetime.now().isoformat(),
            'health_score': self._calculate_health_score(data),
            'maintenance_required': self._check_maintenance_required(data)
        }
    
    def _process_quality_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process quality system data."""
        return {
            'type': 'quality_data',
            'data': data,
            'timestamp': datetime.now().isoformat(),
            'quality_score': data.get('quality_score', 0),
            'compliance_status': data.get('compliance', 'unknown')
        }
    
    def _process_alerts(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process alert data."""
        return {
            'type': 'alert',
            'data': data,
            'timestamp': datetime.now().isoformat(),
            'severity': data.get('severity', 'info'),
            'requires_acknowledgment': data.get('severity') in ['critical', 'warning']
        }
    
    def _process_generic_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process generic data."""
        return {
            'type': 'generic',
            'data': data,
            'timestamp': datetime.now().isoformat()
        }
    
    async def _distribute_data(self, source_name: str, processed_data: Dict[str, Any]):
        """Distribute processed data to subscribed WebSocket connections."""
        if source_name not in self.connection_subscriptions:
            return
        
        subscribers = self.connection_subscriptions[source_name]
        
        for connection_id in subscribers:
            if connection_id in self.websocket_connections:
                try:
                    await self._send_to_websocket(connection_id, processed_data)
                except Exception as e:
                    self.logger.error(f"Failed to send data to {connection_id}: {e}")
    
    async def _send_to_websocket(self, connection_id: str, data: Dict[str, Any]):
        """Send data to specific WebSocket connection."""
        start_time = time.time()
        
        try:
            connection_info = self.websocket_connections[connection_id]
            websocket = connection_info['websocket']
            
            # Apply filters if configured
            if connection_info['filters']:
                data = self._apply_filters(data, connection_info['filters'])
                if not data:  # Data filtered out
                    return
            
            # Compress if enabled
            message = json.dumps(data)
            if connection_info['compression']:
                # In real implementation, would use gzip compression
                pass
            
            # Send message (pseudo-code for WebSocket send)
            # await websocket.send(message)
            
            send_time = (time.time() - start_time) * 1000
            
            # Update connection metrics
            connection_info['messages_sent'] += 1
            connection_info['bytes_sent'] += len(message)
            connection_info['last_activity'] = datetime.now().isoformat()
            
            # Performance validation
            if send_time > self.websocket_response_target_ms:
                self.logger.warning(f"WebSocket send exceeded target: {send_time:.1f}ms")
            
        except Exception as e:
            self.logger.error(f"WebSocket send error: {e}")
            # Mark connection for cleanup
            await self._cleanup_connection(connection_id)
    
    def _validate_subscription_config(self, config: Dict[str, Any]) -> bool:
        """Validate subscription configuration."""
        try:
            # Check required fields
            if 'data_sources' not in config:
                return False
            
            # Check data sources exist
            for source in config['data_sources']:
                if source not in self.data_sources:
                    return False
            
            # Check update frequency is reasonable
            frequency = config.get('update_frequency', 20)
            if frequency < 1 or frequency > 100:
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"Subscription validation error: {e}")
            return False
    
    def _determine_priority(self, data: Dict[str, Any], data_type: str) -> str:
        """Determine data priority for routing."""
        if data_type == 'ai':
            confidence = data.get('confidence', 0)
            if confidence > 0.9:
                return 'high'
            elif confidence > 0.7:
                return 'medium'
            else:
                return 'low'
        
        return 'medium'
    
    def _calculate_trend(self, data: Dict[str, Any]) -> str:
        """Calculate data trend direction."""
        current = data.get('current_value', 0)
        previous = data.get('previous_value', 0)
        
        if current > previous * 1.05:  # 5% increase
            return 'increasing'
        elif current < previous * 0.95:  # 5% decrease
            return 'decreasing'
        else:
            return 'stable'
    
    def _determine_alert_level(self, data: Dict[str, Any]) -> str:
        """Determine alert level for data."""
        value = data.get('value', 0)
        target = data.get('target', 100)
        
        if target > 0:
            ratio = value / target
            if ratio < 0.8:
                return 'critical'
            elif ratio < 0.9:
                return 'warning'
            else:
                return 'normal'
        
        return 'normal'
    
    def _calculate_health_score(self, data: Dict[str, Any]) -> float:
        """Calculate equipment health score."""
        # Simplified health score calculation
        temperature = data.get('temperature', 25)
        vibration = data.get('vibration', 0)
        efficiency = data.get('efficiency', 100)
        
        # Normalize and weight factors
        temp_score = max(0, min(1, (85 - temperature) / 60))  # 25-85Â°C range
        vib_score = max(0, min(1, (1 - vibration)))  # 0-1 vibration range  
        eff_score = efficiency / 100
        
        return (temp_score * 0.3 + vib_score * 0.3 + eff_score * 0.4)
    
    def _check_maintenance_required(self, data: Dict[str, Any]) -> bool:
        """Check if maintenance is required based on data."""
        health_score = self._calculate_health_score(data)
        return health_score < 0.7
    
    def _apply_filters(self, data: Dict[str, Any], filters: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Apply filters to data before sending."""
        try:
            # Priority filter
            if 'min_priority' in filters:
                data_priority = data.get('priority', 'medium')
                min_priority = filters['min_priority']
                
                priority_levels = {'low': 1, 'medium': 2, 'high': 3}
                if priority_levels.get(data_priority, 2) < priority_levels.get(min_priority, 2):
                    return None
            
            # Data type filter
            if 'data_types' in filters:
                data_type = data.get('type', 'generic')
                if data_type not in filters['data_types']:
                    return None
            
            # Value range filter
            if 'value_range' in filters:
                value = data.get('data', {}).get('value')
                if value is not None:
                    min_val = filters['value_range'].get('min')
                    max_val = filters['value_range'].get('max')
                    
                    if min_val is not None and value < min_val:
                        return None
                    if max_val is not None and value > max_val:
                        return None
            
            return data
            
        except Exception as e:
            self.logger.error(f"Filter application error: {e}")
            return data
    
    async def _cleanup_connection(self, connection_id: str):
        """Clean up failed WebSocket connection."""
        try:
            if connection_id in self.websocket_connections:
                connection_info = self.websocket_connections[connection_id]
                
                # Remove from subscriptions
                for data_source in connection_info['subscriptions']:
                    if data_source in self.connection_subscriptions:
                        if connection_id in self.connection_subscriptions[data_source]:
                            self.connection_subscriptions[data_source].remove(connection_id)
                
                # Remove connection
                del self.websocket_connections[connection_id]
                
                with self.lock:
                    self.pipeline_metrics['active_connections'] -= 1
                
                self.logger.info(f"Cleaned up connection: {connection_id}")
                
        except Exception as e:
            self.logger.error(f"Connection cleanup error: {e}")
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get data pipeline performance metrics."""
        with self.lock:
            # Update uptime
            self.pipeline_metrics['uptime_seconds'] = (datetime.now() - self.start_time).total_seconds()
            
            return {
                'data_pipeline_metrics': self.pipeline_metrics.copy(),
                'performance_targets': {
                    'data_latency_target_ms': self.data_latency_target_ms,
                    'websocket_response_target_ms': self.websocket_response_target_ms,
                    'buffer_processing_target_ms': self.buffer_processing_target_ms
                },
                'pipeline_status': self.status,
                'active_connections': len(self.websocket_connections),
                'data_sources': len(self.data_sources),
                'worker_threads': len(self.worker_threads),
                'timestamp': datetime.now().isoformat()
            }
    
    async def validate_data_pipeline(self) -> Dict[str, Any]:
        """Validate data pipeline functionality and performance."""
        validation_results = {
            'engine_name': 'RealTimeDataPipeline',
            'validation_timestamp': datetime.now().isoformat(),
            'tests': {},
            'performance_metrics': {},
            'overall_status': 'unknown'
        }
        
        try:
            # Test 1: Data Push Performance
            test_data = {'value': 42.5, 'timestamp': datetime.now().isoformat()}
            push_result = await self.push_data_to_pipeline('production_system', test_data)
            
            validation_results['tests']['data_push'] = {
                'status': 'pass' if push_result['success'] else 'fail',
                'processing_time_ms': push_result.get('processing_time_ms', 0),
                'target_ms': self.buffer_processing_target_ms,
                'details': f"Pushed data with ID: {push_result.get('data_id', 'unknown')}"
            }
            
            # Test 2: WebSocket Registration
            mock_websocket = {'mock': True}  # Mock WebSocket object
            subscription_config = {
                'data_sources': ['production_system'],
                'update_frequency': 10,
                'filters': {}
            }
            
            reg_result = await self.register_websocket_connection(
                'validation_connection',
                mock_websocket,
                subscription_config
            )
            
            validation_results['tests']['websocket_registration'] = {
                'status': 'pass' if reg_result['success'] else 'fail',
                'registration_time_ms': reg_result.get('registration_time_ms', 0),
                'target_ms': self.websocket_response_target_ms,
                'details': f"Registered connection: {reg_result.get('connection_id', 'unknown')}"
            }
            
            # Test 3: Pipeline Status
            status_test = self.status == DataPipelineStatus.ACTIVE
            
            validation_results['tests']['pipeline_status'] = {
                'status': 'pass' if status_test else 'fail',
                'current_status': self.status,
                'details': f"Pipeline status: {self.status}"
            }
            
            # Performance metrics
            validation_results['performance_metrics'] = self.get_performance_metrics()
            
            # Overall status
            passed_tests = sum(1 for test in validation_results['tests'].values() 
                             if test['status'] == 'pass')
            total_tests = len(validation_results['tests'])
            
            validation_results['overall_status'] = 'pass' if passed_tests == total_tests else 'fail'
            validation_results['test_summary'] = f"{passed_tests}/{total_tests} tests passed"
            
            self.logger.info(f"Data pipeline validation completed: {validation_results['test_summary']}")
            
        except Exception as e:
            validation_results['overall_status'] = 'error'
            validation_results['error'] = str(e)
            self.logger.error(f"Data pipeline validation failed: {e}")
        
        return validation_results
    
    def shutdown(self):
        """Shutdown data pipeline and cleanup resources."""
        try:
            self.status = DataPipelineStatus.SHUTDOWN
            
            # Close all WebSocket connections
            for connection_id in list(self.websocket_connections.keys()):
                asyncio.run(self._cleanup_connection(connection_id))
            
            # Clear data structures
            self.data_buffers.clear()
            self.processing_queues.clear()
            
            # Shutdown executor
            self.executor.shutdown(wait=True)
            
            self.logger.info("Data pipeline shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during data pipeline shutdown: {e}")


# Integration functions for data pipeline
async def integrate_with_ai_layer(data_pipeline: RealTimeDataPipeline, ai_engines: Dict) -> Dict[str, Any]:
    """Integrate data pipeline with AI layer for real-time predictions."""
    try:
        integration_status = {
            'integration_type': 'pipeline_ai',
            'status': 'connected',
            'ai_engines': list(ai_engines.keys()),
            'data_flow': 'bidirectional',
            'capabilities': [
                'real_time_prediction_streaming',
                'ai_alert_distribution',
                'performance_metrics_collection'
            ]
        }
        
        # Set up data push handlers for each AI engine
        for engine_name, engine in ai_engines.items():
            # In a real implementation, would set up callbacks
            pass
        
        return integration_status
        
    except Exception as e:
        return {
            'integration_type': 'pipeline_ai',
            'status': 'error',
            'error': str(e)
        }